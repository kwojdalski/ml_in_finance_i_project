{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e9e37b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Stock Market Movement Prediction\n",
    "### Using Machine Learning to Forecast Next-Day Stock Price Movements\n",
    "This project tackles a binary classification problem in financial markets -\n",
    "predicting whether individual US stocks will move up or down the following day.\n",
    "The goal is to develop a model that can assist in making data-driven investment decisions.\n",
    "The model uses 20 days of historical price returns and trading volumes, along with\n",
    "categorical stock metadata like industry and sector classifications, to identify\n",
    "predictive patterns in market behavior.\n",
    "The public benchmark accuracy of 51.31% was achieved using a Random Forest model that considered\n",
    "the previous 5 days of data along with the average sector returns from the prior day.\n",
    "\n",
    "#### Agenda\n",
    "1. **Data Preprocessing**\n",
    "   - Loading training and test datasets\n",
    "   - Handling missing values and target encoding\n",
    "   - Feature engineering (technical indicators of different types)\n",
    "2. **Model Implementation and Evaluation**\n",
    "   - Decision Tree Classifier\n",
    "      - Baseline model (accuracy: 0.510)\n",
    "      - Tuned model with hyperparameter optimization (accuracy: 0.5325)\n",
    "   - XGBoost Classifier\n",
    "      - Baseline model (accuracy: 0.53)\n",
    "      - Tuned model with hyperparameter optimization (accuracy: 0.8775)\n",
    "   - Neural Network\n",
    "      - Accuracy: 0.5144\n",
    "3. **Model Comparison**\n",
    "   - Cross-validation results\n",
    "   - Feature importance analysis\n",
    "   - ROC curves and confusion matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43132e30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Data description\n",
    "\n",
    "3 datasets are provided as csv files, split between training inputs and outputs, and test inputs.\n",
    "\n",
    "Input datasets comprise 47 columns: the first ID column contains unique row identifiers while the other 46 descriptive features correspond to:\n",
    "\n",
    "* **DATE**: an index of the date (the dates are randomized and anonymized so there is no continuity or link between any dates),\n",
    "* **STOCK**: an index of the stock,\n",
    "* **INDUSTRY**: an index of the stock industry domain (e.g., aeronautic, IT, oil company),\n",
    "* **INDUSTRY_GROUP**: an index of the group industry,\n",
    "* **SUB_INDUSTRY**: a lower level index of the industry,\n",
    "* **SECTOR**: an index of the work sector,\n",
    "* **RET_1 to RET_20**: the historical residual returns among the last 20 days (i.e., RET_1 is the return of the previous day and so on),\n",
    "* **VOLUME_1 to VOLUME_20**: the historical relative volume traded among the last 20 days (i.e., VOLUME_1 is the relative volume of the previous day and so on),\n",
    "\n",
    "Output datasets are only composed of 2 columns:\n",
    "\n",
    "* **ID**: the unique row identifier (corresponding to the input identifiers)\n",
    "and the binary target:\n",
    "* **RET**: the sign of the residual stock return at time $t$\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "The one-day return of a stock :\n",
    "$$R^t = \\frac{P_j^t}{P_j^{t-1}} - 1$$\n",
    "\n",
    "The volume is the ratio of the stock volume to the median volume of the past 20 days.\n",
    "The relative volumes are computed using the median of the past 20 days' volumes.\n",
    "If any day within this 20-day window has a missing volume value, it will cause NaN values in the calculation for subsequent days.\n",
    "For example, if there is a missing value on day $D$, then the relative volumes for days $D$ to $D+19$ will be affected.\n",
    "\n",
    "The relative volume $\\tilde{V}^t_j$ at time $t$ of a stock $j$ is calculated as:\n",
    "$$\n",
    "\\tilde{V}^t_j = \\frac{V^t_j}{\\text{median}( \\{ V^{t-1}_j, \\ldots, V^{t-20}_j \\} )}\n",
    "$$\n",
    "\n",
    "The adjusted relative volume $V^t_j$ is then given by:\n",
    "$$\n",
    "V^t_j = \\tilde{V}^t_j - \\frac{1}{n} \\sum_{i=1}^{n} \\tilde{V}^t_i\n",
    "$$\n",
    "------------------------------------------------------------------------------------------------\n",
    "Guidelines from the organizers:\n",
    "The solution files submitted by participants shall follow this output dataset format (i.e contain only two columns, ID and RET, where the ID values correspond to the input test data).\n",
    "An example submission file containing random predictions is provided.\n",
    "\n",
    "**418595 observations (i.e. lines) are available for the training datasets while 198429 observations are used for the test datasets.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b06f86",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Implementation Steps\n",
    "\n",
    "This notebook implements the following steps:\n",
    "\n",
    "1. **Data Loading and Preprocessing**\n",
    "   - Load training and test datasets\n",
    "   - Handle missing values and data cleaning\n",
    "   - Calculate technical indicators using TA-Lib\n",
    "   - Filter out infinity values and remove duplicated columns\n",
    "   - Split data into training and test sets (75%/25% split)\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Calculate technical indicators like RSI, OBV, EMA etc.\n",
    "   - Save indicators to pickle files for reuse\n",
    "   - Drop unnecessary ID and categorical columns\n",
    "   - Remove redundant technical indicators\n",
    "\n",
    "3. **Model Development and Tuning**\n",
    "   - Decision Tree Classifier\n",
    "      - Baseline model (accuracy: 0.510)\n",
    "      - Tuned model with hyperparameters (accuracy: 0.533)\n",
    "   - Gradient Boosting\n",
    "      - Stepwise tuning of n_estimators, tree params, leaf params\n",
    "      - Best model achieves significant improvement\n",
    "   - Neural Network\n",
    "      - Simple feed-forward architecture\n",
    "      - Training with BCE loss and Adam optimizer\n",
    "\n",
    "4. **Model Comparison and Analysis**\n",
    "   - Compare accuracy across all models\n",
    "   - Analyze feature importance\n",
    "   - Key findings on model performance and technical indicators\n",
    "   - Discussion of overfitting and benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d2627",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828533c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(__file__).parent.parent\n",
    "path = path / \"src\"\n",
    "sys.path.append(str(path))\n",
    "\n",
    "import kedro.ipython\n",
    "\n",
    "kedro.ipython.load_ipython_extension(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248032fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "from kedro.framework.session import KedroSession\n",
    "\n",
    "from src.ml_in_finance_i_project.utils import get_node_idx, get_node_outputs\n",
    "\n",
    "\n",
    "def run_pipeline_node(pipeline_name: str, node_name: str, inputs: dict) -> dict:\n",
    "    \"\"\"Run a specific node from a pipeline.\"\"\"\n",
    "    with KedroSession.create() as session:\n",
    "        context = session.load_context()\n",
    "        pipeline = context.pipelines[pipeline_name]\n",
    "        node = [n for n in pipeline.nodes if n.name == node_name][0]\n",
    "        return node.run(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e351d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "x_train_raw = catalog.load(\"x_train_raw\")\n",
    "y_train_raw = catalog.load(\"y_train_raw\")\n",
    "x_test_raw = catalog.load(\"x_test_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2f53b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Checking and configuring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Colab used to speed up the computation in xgboost model\n",
    "## warning: this function must be run before importing libraries\n",
    "# if running in Google Colab\n",
    "def setup_colab_environment():\n",
    "    \"\"\"\n",
    "    Set up Google Colab environment by mounting drive and creating symlinks.\n",
    "    Returns True if running in Colab, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import os\n",
    "\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        req_symlinks = [\n",
    "            (\"data\", \"ml_in_finance_i_project/data\"),\n",
    "            (\"src\", \"ml_in_finance_i_project/src\"),\n",
    "        ]\n",
    "        # Create symlinks if they don't exist\n",
    "        for dest, src in req_symlinks:\n",
    "            if not os.path.exists(dest):\n",
    "                os.symlink(f\"/content/drive/Othercomputers/My Mac/{src}\", dest)\n",
    "        return True\n",
    "\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b688c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a specific node from a pipeline.\n",
    "def run_pipeline_node(pipeline_name: str, node_name: str, inputs: dict):\n",
    "    \"\"\"Run a specific node from a pipeline.\n",
    "\n",
    "    Args:\n",
    "        pipeline_name: Name of the pipeline\n",
    "        node_name: Name of the node to run\n",
    "        inputs: Dictionary of input parameters for the node\n",
    "\n",
    "    Returns:\n",
    "        Output from running the node\n",
    "    \"\"\"\n",
    "    node_idx = get_node_idx(pipelines[pipeline_name], node_name)\n",
    "    return pipelines[pipeline_name].nodes[node_idx].run(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81612e8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "IN_COLAB = setup_colab_environment()\n",
    "\n",
    "# Configure logging to stdout\n",
    "log.basicConfig(\n",
    "    level=log.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[log.StreamHandler()],\n",
    ")\n",
    "conf_params = context.config_loader.get(\"parameters\")\n",
    "target = conf_params[\"model_options\"][\"target\"]\n",
    "kfold = conf_params[\"model_options\"][\"kfold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67648c94",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484c999",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set data directory based on environment\n",
    "# Run data processing pipeline node\n",
    "out = run_pipeline_node(\n",
    "    \"data_processing\",\n",
    "    \"load_data_node\",\n",
    "    {\n",
    "        \"x_train_raw\": x_train_raw,\n",
    "        \"y_train_raw\": y_train_raw,\n",
    "        \"x_test_raw\": x_test_raw,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcdb4f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Problem visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e1008",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot returns and volume\n",
    "run_pipeline_node(\n",
    "    \"reporting\",\n",
    "    \"plot_returns_volume_node\",\n",
    "    {\"train_df\": out[\"train_df\"], \"params:example_row_id\": 28},\n",
    ")[\"returns_volume_plot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d10a8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Head of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216958ba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "out[\"test_df\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a531ec",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Info about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Dataset Info:\")\n",
    "out[\"train_df\"].info()\n",
    "print(\"\\nTest Dataset Info:\")\n",
    "out[\"test_df\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193be42",
   "metadata": {},
   "source": [
    "#### Plot nan percentages across categorical var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a940b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline_node(\n",
    "    \"reporting\",\n",
    "    \"plot_nan_percentages_node\",\n",
    "    {\"cleaned_train\": out[\"train_df\"]},\n",
    ")[\"nan_percentages_plot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5facb4e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "#### Possible reasons for missing values:\n",
    "1. Market closures (market data might be missing for weekends and public holidays) - some dates clearly show a very high\n",
    "percentage of missing values\n",
    "2. Data collection issues (for instance, market data might come from different US venues, e.g. NYSE, NASDAQ, CBOE, etc.)\n",
    "3. Randomization and anonymization of dates\n",
    "4. The way relative volumes are calculated (one day missing causes missing values for the next 19 days) - could have something to do with calculating volumes on weekends / public holidays\n",
    "5. Done on purpose by the organizers to make the problem more challenging\n",
    "6. Some stocks might be delisted or suspended from trading (reference data problem) - some stocks in fact have up to 100% missing values\n",
    "7. Some stocks might be barely trading (either due to low volume or in a non-continuous manner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04155235",
   "metadata": {
    "title": "Dropping rows with NAs for the most important variables (RET_1 to RET_5)"
   },
   "outputs": [],
   "source": [
    "# The assumption is that the most recent values for regressors are the most important\n",
    "\n",
    "out2 = run_pipeline_node(\n",
    "    \"data_processing\", \"drop_missing_returns_node\", {\"train_df\": out[\"train_df\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c469b1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Preprocessing data\n",
    "* Dropping rows with NAs\n",
    "* If arg set to true, removing ID columns\n",
    "* RET is encoded from bool to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84faf628",
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = run_pipeline_node(\n",
    "    \"data_processing\",\n",
    "    \"preprocess_data_node\",\n",
    "    {\n",
    "        \"train_df_dropped\": out2[\"train_df_dropped\"],\n",
    "        \"test_df\": out[\"test_df\"],\n",
    "        \"params:remove_id_cols\": conf_params[\"remove_id_cols\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be64978",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Check Class Imbalance\n",
    "\n",
    "**Class Imbalance**:\n",
    "Classes seem to be balanced almost perfectly. This is expected, as the target variable is the sign of the return.\n",
    "Intuitively, it is expected that the sign of the return is more likely to be positive (by a small margin) than negative\n",
    "unless data comes from a bear market period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81e410",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "md(\n",
    "    f\"Class imbalance: {out3['train_df_preprocessed']['RET'].value_counts(normalize=True)[0] * 100:.2f}%\"\n",
    "    + f\" {out3['train_df_preprocessed']['RET'].value_counts(normalize=True)[1] * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420e087",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Plot correlation matrix\n",
    "\n",
    "Findings:\n",
    "* Most stock returns are nearly not correlated with each other (this is expected).\n",
    "Otherwise, someone could make a lot of money\n",
    "by exploiting this non-subtle pattern.\n",
    "    * Eventually, excess alpha would converge to 0\n",
    "* Among stock returns the strongest correlation is within stock returns adjacent to each other (e.g. $RET_1$ and $RET_2$)\n",
    "    * This is expected as the magnitude return of a stock is likely to be correlated with the magnitude stock return of the previous day\n",
    "* Volumes are highly correlated (this is kind of expected) due to the way $VOLUME_i$ variables are calculated.\n",
    "Moreover, Volatility and Volumes tend to cluster. Hence, correlation is positive.\n",
    "* There is a strong positive correlation between the volume of the previous day and the return of the following day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926db37",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "out_corr = run_pipeline_node(\n",
    "    \"reporting\",\n",
    "    \"plot_correlation_matrix_node\",\n",
    "    {\"train_df\": out3[\"train_df_preprocessed\"]},\n",
    ")\n",
    "out_corr[\"correlation_matrix_plot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ba471",
   "metadata": {},
   "source": [
    "## Feature Engineering - Technical Indicators using TA-Lib\n",
    "In this part, we calculate the technical indicators for the train and test data.\n",
    "We save the results in pickle files to avoid recalculating them every time.\n",
    "The following functions inside the function are used:\n",
    "- talib.OBV, {\"data_type\": \"both\"}),\n",
    "- talib.RSI, {\"data_type\": \"ret\"}),\n",
    "- talib.MOM, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.ROCR, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.CMO, {\"timeperiod\": 14, \"data_type\": \"ret\"}),\n",
    "- talib.EMA, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.SMA, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.WMA, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.MIDPOINT, {\"timeperiod\": 10, \"data_type\": \"ret\"}),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59403111",
   "metadata": {},
   "source": [
    "#### Feature engineering - cont'd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from organizers' notebook, it's an extended version of variables they used\n",
    "# Feature engineering\n",
    "# Calculate statistical features\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    out4 = run_pipeline_node(\n",
    "        \"data_processing\",\n",
    "        \"calculate_statistical_features_node\",\n",
    "        {\n",
    "            \"train_df_preprocessed\": out3[\"train_df_preprocessed\"],\n",
    "            \"test_df_preprocessed\": out3[\"test_df_preprocessed\"],\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c032e41",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Calculate technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7e298",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "calculate = False\n",
    "\n",
    "if calculate:\n",
    "    out5 = run_pipeline_node(\n",
    "        \"data_processing\",\n",
    "        \"calculate_technical_indicators_node\",\n",
    "        {\n",
    "            \"train_df_statistical_features\": out4[\"train_df_statistical_features\"],\n",
    "            \"test_df_statistical_features\": out4[\"test_df_statistical_features\"],\n",
    "            \"params:features_ret_vol\": conf_params[\"features_ret_vol\"],\n",
    "        },\n",
    "    )\n",
    "else:\n",
    "    out5 = get_node_outputs(\n",
    "        pipelines[\"data_processing\"].nodes[\n",
    "            get_node_idx(\n",
    "                pipelines[\"data_processing\"], \"calculate_technical_indicators_node\"\n",
    "            )\n",
    "        ],\n",
    "        catalog,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5dff6c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Columns to drop\n",
    "They could bring in some predictive power, but we don't want to use them in this case\n",
    "as the scope is limited for this project\n",
    "['ID', 'STOCK', 'DATE', 'INDUSTRY', 'INDUSTRY_GROUP', 'SECTOR', 'SUB_INDUSTRY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c054b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out6 = run_pipeline_node(\n",
    "    \"data_processing\",\n",
    "    \"drop_id_cols_node\",\n",
    "    {\n",
    "        \"train_ta_indicators\": out5[\"train_ta_indicators\"],\n",
    "        \"test_ta_indicators\": out5[\"test_ta_indicators\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4b8f6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Assumption is that probably some technical indicators are not useful for the prediction.\n",
    "For instance SMA(10), SMA(11) etc. dont give any information in the context of RET.\n",
    "It's an arbitrary choice, but we want to keep the number of features low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out7 = run_pipeline_node(\n",
    "    \"data_processing\",\n",
    "    \"drop_obsolete_technical_indicators_node\",\n",
    "    {\n",
    "        \"train_ta_indicators_dropped\": out6[\"train_ta_indicators_dropped\"],\n",
    "        \"test_ta_indicators_dropped\": out6[\"test_ta_indicators_dropped\"],\n",
    "        \"params:target\": conf_params[\"model_options\"][\"target\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27072cc2",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Filter out infinity values"
   },
   "outputs": [],
   "source": [
    "out8 = run_pipeline_node(\n",
    "    \"data_processing\",\n",
    "    \"filter_infinity_values_node\",\n",
    "    {\n",
    "        \"train_df_technical_indicators\": out7[\"train_df_technical_indicators\"],\n",
    "        \"test_df_technical_indicators\": out7[\"test_df_technical_indicators\"],\n",
    "        \"params:target\": conf_params[\"model_options\"][\"target\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf8525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns\n",
    "out9 = run_pipeline_node(\n",
    "    \"data_processing\",\n",
    "    \"remove_duplicated_columns_node\",\n",
    "    {\n",
    "        \"train_df_filtered\": out8[\"train_df_filtered\"],\n",
    "        \"test_df_filtered\": out8[\"test_df_filtered\"],\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
