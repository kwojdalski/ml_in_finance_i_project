{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ddba05",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "# Stock Market Movement Prediction\n",
    "### Using Machine Learning to Forecast Next-Day Stock Price Movements\n",
    "This project tackles a binary classification problem in financial markets -\n",
    "predicting whether individual US stocks will move up or down the following day.\n",
    "The goal is to develop a model that can assist in making data-driven investment decisions.\n",
    "The model uses 20 days of historical price returns and trading volumes, along with\n",
    "categorical stock metadata like industry and sector classifications, to identify\n",
    "predictive patterns in market behavior.\n",
    "The public benchmark accuracy of 51.31% was achieved using a Random Forest model that considered\n",
    "the previous 5 days of data along with the average sector returns from the prior day.\n",
    "\n",
    "#### Agenda\n",
    "1. **Data Preprocessing**\n",
    "   - Loading training and test datasets\n",
    "   - Handling missing values and target encoding\n",
    "   - Feature engineering (technical indicators of different types)\n",
    "2. **Model Implementation and Evaluation**\n",
    "   - Decision Tree Classifier\n",
    "      - Baseline model (accuracy: 0.510)\n",
    "      - Tuned model with hyperparameter optimization (accuracy: 0.5325)\n",
    "   - XGBoost Classifier\n",
    "      - Baseline model (accuracy: 0.53)\n",
    "      - Tuned model with hyperparameter optimization (accuracy: 0.8775)\n",
    "   - Neural Network\n",
    "      - Accuracy: 0.5144\n",
    "3. **Model Comparison**\n",
    "   - Cross-validation results\n",
    "   - Feature importance analysis\n",
    "   - ROC curves and confusion matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d20ba9",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": ["documentation"]
   },
   "source": [
    "## Data description\n",
    "\n",
    "3 datasets are provided as csv files, split between training inputs and outputs, and test inputs.\n",
    "\n",
    "Input datasets comprise 47 columns: the first ID column contains unique row identifiers while the other 46 descriptive features correspond to:\n",
    "\n",
    "* **DATE**: an index of the date (the dates are randomized and anonymized so there is no continuity or link between any dates),\n",
    "* **STOCK**: an index of the stock,\n",
    "* **INDUSTRY**: an index of the stock industry domain (e.g., aeronautic, IT, oil company),\n",
    "* **INDUSTRY_GROUP**: an index of the group industry,\n",
    "* **SUB_INDUSTRY**: a lower level index of the industry,\n",
    "* **SECTOR**: an index of the work sector,\n",
    "* **RET_1 to RET_20**: the historical residual returns among the last 20 days (i.e., RET_1 is the return of the previous day and so on),\n",
    "* **VOLUME_1 to VOLUME_20**: the historical relative volume traded among the last 20 days (i.e., VOLUME_1 is the relative volume of the previous day and so on),\n",
    "\n",
    "Output datasets are only composed of 2 columns:\n",
    "\n",
    "* **ID**: the unique row identifier (corresponding to the input identifiers)\n",
    "and the binary target:\n",
    "* **RET**: the sign of the residual stock return at time $t$\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "The one-day return of a stock :\n",
    "$$R^t = \\frac{P_j^t}{P_j^{t-1}} - 1$$\n",
    "\n",
    "The volume is the ratio of the stock volume to the median volume of the past 20 days.\n",
    "The relative volumes are computed using the median of the past 20 days' volumes.\n",
    "If any day within this 20-day window has a missing volume value, it will cause NaN values in the calculation for subsequent days.\n",
    "For example, if there is a missing value on day $D$, then the relative volumes for days $D$ to $D+19$ will be affected.\n",
    "\n",
    "The relative volume $\\tilde{V}^t_j$ at time $t$ of a stock $j$ is calculated as:\n",
    "$$\n",
    "\\tilde{V}^t_j = \\frac{V^t_j}{\\text{median}( \\{ V^{t-1}_j, \\ldots, V^{t-20}_j \\} )}\n",
    "$$\n",
    "\n",
    "The adjusted relative volume $V^t_j$ is then given by:\n",
    "$$\n",
    "V^t_j = \\tilde{V}^t_j - \\frac{1}{n} \\sum_{i=1}^{n} \\tilde{V}^t_i\n",
    "$$\n",
    "------------------------------------------------------------------------------------------------\n",
    "Guidelines from the organizers:\n",
    "The solution files submitted by participants shall follow this output dataset format (i.e contain only two columns, ID and RET, where the ID values correspond to the input test data).\n",
    "An example submission file containing random predictions is provided.\n",
    "\n",
    "**418595 observations (i.e. lines) are available for the training datasets while 198429 observations are used for the test datasets.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ac7e5",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": ["documentation"]
   },
   "source": [
    "## Implementation Steps\n",
    "\n",
    "This notebook implements the following steps:\n",
    "\n",
    "1. **Data Loading and Preprocessing**\n",
    "   - Load training and test datasets\n",
    "   - Handle missing values and data cleaning\n",
    "   - Calculate technical indicators using TA-Lib\n",
    "   - Filter out infinity values and remove duplicated columns\n",
    "   - Split data into training and test sets (75%/25% split)\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Calculate technical indicators like RSI, OBV, EMA etc.\n",
    "   - Save indicators to pickle files for reuse\n",
    "   - Drop unnecessary ID and categorical columns\n",
    "   - Remove redundant technical indicators\n",
    "\n",
    "3. **Model Development and Tuning**\n",
    "   - Decision Tree Classifier\n",
    "      - Baseline model (accuracy: 0.510)\n",
    "      - Tuned model with hyperparameters (accuracy: 0.533)\n",
    "   - Gradient Boosting\n",
    "      - Stepwise tuning of n_estimators, tree params, leaf params\n",
    "      - Best model achieves significant improvement\n",
    "   - Neural Network\n",
    "      - Simple feed-forward architecture\n",
    "      - Training with BCE loss and Adam optimizer\n",
    "\n",
    "4. **Model Comparison and Analysis**\n",
    "   - Compare accuracy across all models\n",
    "   - Analyze feature importance\n",
    "   - Key findings on model performance and technical indicators\n",
    "   - Discussion of overfitting and benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c0bf9",
   "metadata": {
    "tags": ["documentation"]
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd40d9",
   "metadata": {
    "tags": ["imports"]
   },
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import re\n",
    "from itertools import compress\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import Markdown as md\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "\n",
    "from src.GBClassifierGridSearch import HistGBClassifierGridSearch\n",
    "from src.nn import Net\n",
    "from src.ta_indicators import (\n",
    "    calculate_all_ta_indicators,\n",
    "    filter_infinity_values,\n",
    "    remove_duplicated_columns,\n",
    ")\n",
    "from src.utils import (\n",
    "    ID_COLS,\n",
    "    feature_importance,\n",
    "    load_data,\n",
    "    model_fit,\n",
    "    plot_correlation_matrix,\n",
    "    plot_model_accuracy,\n",
    "    plot_nan_percentages,\n",
    "    plot_ret_and_vol,\n",
    "    preprocess_data,\n",
    "    simulate_strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386d9f1",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": ["documentation"]
   },
   "source": [
    "### Checking and configuring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53e312",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": ["setup"]
   },
   "outputs": [],
   "source": [
    "## Google Colab used to speed up the computation in xgboost model\n",
    "## warning: this function must be run before importing libraries\n",
    "# if running in Google Colab\n",
    "def setup_colab_environment():\n",
    "    \"\"\"\n",
    "    Set up Google Colab environment by mounting drive and creating symlinks.\n",
    "    Returns True if running in Colab, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import os\n",
    "\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        req_symlinks = [\n",
    "            (\"data\", \"ml_in_finance_i_project/data\"),\n",
    "            (\"src\", \"ml_in_finance_i_project/src\"),\n",
    "        ]\n",
    "        # Create symlinks if they don't exist\n",
    "        for dest, src in req_symlinks:\n",
    "            if not os.path.exists(dest):\n",
    "                os.symlink(f\"/content/drive/Othercomputers/My Mac/{src}\", dest)\n",
    "        return True\n",
    "\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "IN_COLAB = setup_colab_environment()\n",
    "\n",
    "# Configure logging to stdout\n",
    "log.basicConfig(\n",
    "    level=log.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[log.StreamHandler()],\n",
    ")\n",
    "target = \"RET\"\n",
    "kfold = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4bdb03",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9262584",
   "metadata": {
    "tags": ["data_loading"]
   },
   "outputs": [],
   "source": [
    "# Set data directory based on environment\n",
    "\n",
    "data_dir = Path(\"/content/data\") if IN_COLAB else Path(\"./data\")\n",
    "x_train_path: Path = data_dir / \"x_train.csv\"\n",
    "y_train_path: Path = data_dir / \"y_train.csv\"\n",
    "x_test_path: Path = data_dir / \"x_test.csv\"\n",
    "train_df, test_df = load_data(x_train_path, y_train_path, x_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d066365",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "#### Problem visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3dfc5",
   "metadata": {
    "tags": ["visualization"]
   },
   "outputs": [],
   "source": [
    "# Plot returns and volume\n",
    "plot_ret_and_vol(train_df, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3f71d",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "#### Head of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8961d0",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["data_exploration"]
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53eff0",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "#### Info about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2abce3",
   "metadata": {
    "tags": ["data_exploration"]
   },
   "outputs": [],
   "source": [
    "print(\"Training Dataset Info:\")\n",
    "train_df.info()\n",
    "print(\"\\nTest Dataset Info:\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da212f7",
   "metadata": {
    "tags": ["documentation"]
   },
   "source": [
    "#### Plot nan percentages across categorical var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472adea",
   "metadata": {
    "tags": ["visualization"]
   },
   "outputs": [],
   "source": [
    "plot_nan_percentages(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e8064",
   "metadata": {
    "tags": ["documentation"]
   },
   "source": [
    "\n",
    "#### Possible reasons for missing values:\n",
    "1. Market closures (market data might be missing for weekends and public holidays) - some dates clearly show a very high\n",
    "percentage of missing values\n",
    "2. Data collection issues (for instance, market data might come from different US venues, e.g. NYSE, NASDAQ, CBOE, etc.)\n",
    "3. Randomization and anonymization of dates\n",
    "4. The way relative volumes are calculated (one day missing causes missing values for the next 19 days) - could have something to do with calculating volumes on weekends / public holidays\n",
    "5. Done on purpose by the organizers to make the problem more challenging\n",
    "6. Some stocks might be delisted or suspended from trading (reference data problem) - some stocks in fact have up to 100% missing values\n",
    "7. Some stocks might be barely trading (either due to low volume or in a non-continuous manner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561eda6",
   "metadata": {
    "tags": ["data_cleaning"],
    "title": "Dropping rows with NAs for the most important variables (RET_1 to RET_5)"
   },
   "outputs": [],
   "source": [
    "# The assumption is that the most recent values for regressors are the most important\n",
    "return_features = [f\"RET_{day}\" for day in range(1, 6)]\n",
    "return_to_drop = train_df[\n",
    "    (train_df[return_features].isna().sum(axis=1) / train_df[return_features].shape[1])\n",
    "    >= 1\n",
    "][return_features]\n",
    "return_to_drop\n",
    "train_df.drop(index=return_to_drop.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64ec8d",
   "metadata": {
    "tags": ["documentation"]
   },
   "source": [
    "#### Preprocessing data\n",
    "* Dropping rows with NAs\n",
    "* If arg set to true, removing ID columns\n",
    "* RET is encoded from bool to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf611306",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["preprocessing"]
   },
   "outputs": [],
   "source": [
    "train_df, test_df = preprocess_data(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    remove_id_cols=False,\n",
    "    # sample_n=50000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d2b62",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "#### Check Class Imbalance\n",
    "\n",
    "**Class Imbalance**:\n",
    "Classes seem to be balanced almost perfectly. This is expected, as the target variable is the sign of the return.\n",
    "Intuitively, it is expected that the sign of the return is more likely to be positive (by a small margin) than negative\n",
    "unless data comes from a bear market period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c29a3",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": ["data_exploration"]
   },
   "outputs": [],
   "source": [
    "md(\n",
    "    f\"Class imbalance: {train_df['RET'].value_counts(normalize=True)[0] * 100:.2f}%\"\n",
    "    + f\" {train_df['RET'].value_counts(normalize=True)[1] * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d888a6a",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "#### Plot correlation matrix\n",
    "\n",
    "Findings:\n",
    "* Most stock returns are nearly not correlated with each other (this is expected).\n",
    "Otherwise, someone could make a lot of money\n",
    "by exploiting this non-subtle pattern.\n",
    "    * Eventually, excess alpha would converge to 0\n",
    "* Among stock returns the strongest correlation is within stock returns adjacent to each other (e.g. $RET_1$ and $RET_2$)\n",
    "    * This is expected as the magnitude return of a stock is likely to be correlated with the magnitude stock return of the previous day\n",
    "* Volumes are highly correlated (this is kind of expected) due to the way $VOLUME_i$ variables are calculated.\n",
    "Moreover, Volatility and Volumes tend to cluster. Hence, correlation is positive.\n",
    "* There is a strong positive correlation between the volume of the previous day and the return of the following day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e5053",
   "metadata": {
    "tags": ["visualization"]
   },
   "outputs": [],
   "source": [
    "plot_correlation_matrix(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0e03b",
   "metadata": {
    "tags": ["documentation"]
   },
   "source": [
    "## Feature Engineering - Technical Indicators using TA-Lib\n",
    "In this part, we calculate the technical indicators for the train and test data.\n",
    "We save the results in pickle files to avoid recalculating them every time.\n",
    "The following functions inside the function are used:\n",
    "- talib.OBV, {\"data_type\": \"both\"}),\n",
    "- talib.RSI, {\"data_type\": \"ret\"}),\n",
    "- talib.MOM, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.ROCR, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.CMO, {\"timeperiod\": 14, \"data_type\": \"ret\"}),\n",
    "- talib.EMA, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.SMA, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.WMA, {\"timeperiod\": 5, \"data_type\": \"ret\"}),\n",
    "- talib.MIDPOINT, {\"timeperiod\": 10, \"data_type\": \"ret\"}),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621709e6",
   "metadata": {
    "tags": ["documentation"]
   },
   "source": [
    "#### Feature engineering - cont'd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b6c7b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": ["feature_engineering"]
   },
   "outputs": [],
   "source": [
    "# This comes from organizers' notebook, it's an extended version of variables they used\n",
    "# Feature engineering\n",
    "new_features = []\n",
    "\n",
    "# Conditional aggregated features\n",
    "shifts = [1, 2, 3, 4, 5]  # Choose some different shifts\n",
    "statistics = [\"median\", \"std\"]  # the type of stat\n",
    "gb_features = [\n",
    "    [\"DATE\", \"INDUSTRY\"],\n",
    "    [\"DATE\", \"INDUSTRY_GROUP\"],\n",
    "    [\"DATE\", \"SECTOR\"],\n",
    "    [\"DATE\", \"SUB_INDUSTRY\"],\n",
    "]\n",
    "target_feature = \"RET\"\n",
    "# Create a name by joining the last element of each gb_feature list\n",
    "for gb_feature in gb_features:\n",
    "    for shift in shifts:\n",
    "        for stat in statistics:\n",
    "            name = f\"{target_feature}_{shift}_{gb_feature[-1]}_{stat}\"\n",
    "            feat = f\"{target_feature}_{shift}\"\n",
    "            new_features.append(name)\n",
    "            for data in [train_df, test_df]:\n",
    "                data[name] = data.groupby(gb_feature)[feat].transform(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934ff51",
   "metadata": {
    "tags": ["feature_engineering"]
   },
   "outputs": [],
   "source": [
    "ta_indicators_path = Path(\"./data/ta_indicators.pkl\")\n",
    "test_ta_indicators_path = Path(\"./data/test_ta_indicators.pkl\")\n",
    "\n",
    "if ta_indicators_path.exists() and test_ta_indicators_path.exists():\n",
    "    log.info(\"Loading pre-calculated technical indicators from pickle files\")\n",
    "    ta_indicators_df = pd.read_pickle(ta_indicators_path)\n",
    "    test_ta_indicators_df = pd.read_pickle(test_ta_indicators_path)\n",
    "else:\n",
    "    log.info(\"Calculating technical indicators for train and test data\")\n",
    "    ta_indicators_df = calculate_all_ta_indicators(train_df)\n",
    "    test_ta_indicators_df = calculate_all_ta_indicators(test_df)\n",
    "    # Save calculated indicators to pickle files\n",
    "    ta_indicators_df.to_pickle(ta_indicators_path)\n",
    "    test_ta_indicators_df.to_pickle(test_ta_indicators_path)\n",
    "    log.info(\n",
    "        f\"Saved technical indicators to {ta_indicators_path} and {test_ta_indicators_path}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd1cd4",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "#### Concatenate the technical indicators to the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99813a",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["feature_engineering"]
   },
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, ta_indicators_df], axis=1)\n",
    "test_df = pd.concat([test_df, test_ta_indicators_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fb116",
   "metadata": {
    "tags": ["data_saving"]
   },
   "outputs": [],
   "source": [
    "# Save/load processed training data\n",
    "train_df_path = Path(\"./data/processed_train_df.pkl\")\n",
    "test_df_path = Path(\"./data/processed_test_df.pkl\")\n",
    "if train_df_path.exists() and test_df_path.exists():\n",
    "    log.info(f\"Loading processed training data from {train_df_path}\")\n",
    "    train_df = pd.read_pickle(train_df_path)\n",
    "    test_df = pd.read_pickle(test_df_path)\n",
    "else:\n",
    "    log.info(f\"Saving processed training data to {train_df_path}\")\n",
    "    train_df.to_pickle(train_df_path)\n",
    "    test_df.to_pickle(test_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7174520",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": ["documentation"]
   },
   "source": [
    "#### Columns to drop\n",
    "They could bring in some predictive power, but we don't want to use them in this case\n",
    "as the scope is limited for this project\n",
    "['ID', 'STOCK', 'DATE', 'INDUSTRY', 'INDUSTRY_GROUP', 'SECTOR', 'SUB_INDUSTRY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9b86f",
   "metadata": {
    "tags": ["feature_selection"]
   },
   "outputs": [],
   "source": [
    "train_df.drop(columns=ID_COLS, inplace=True, errors=\"ignore\")\n",
    "test_df.drop(columns=ID_COLS, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb600e",
   "metadata": {},
   "source": [
    "Assumption is that probably some technical indicators are not useful for the prediction.\n",
    "For instance SMA(10), SMA(11) etc. dont give any information in the context of RET.\n",
    "It's an arbitrary choice, but we want to keep the number of features low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cb700",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Further data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5f010",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    col\n",
    "    for col in train_df.columns\n",
    "    if re.search(\n",
    "        r\"\\D(?:([1-2]{1}[0-9])|([8-9]{1})\\_)\",\n",
    "        str(col),\n",
    "    )\n",
    "    and not col.startswith((\"RET\", \"VOLUME\"))  # don't drop RET and VOLUME\n",
    "]\n",
    "\n",
    "train_df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ecbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features before using them\n",
    "features = [col for col in train_df.columns if col != target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a287088",
   "metadata": {
    "title": "Filter out infinity values"
   },
   "outputs": [],
   "source": [
    "train_df, test_df, features = filter_infinity_values(\n",
    "    train_df, test_df, features, target\n",
    ")\n",
    "\n",
    "# Remove duplicated columns\n",
    "train_df, test_df, features = remove_duplicated_columns(train_df, test_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "# Default selection\n",
    "features = train_df.columns.drop(target).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c871bf",
   "metadata": {},
   "source": [
    "## ML DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c6034",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Train and test set splitting\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    train_df[features],\n",
    "    train_df[\"RET\"],\n",
    "    test_size=0.25,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decison tree baseline model\n",
    "base_dt = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe3deb",
   "metadata": {},
   "source": [
    "#### Fit the model\n",
    "`model_fit()` function a model, makes predictions, and evaluates performance\n",
    "using confusion matrix, accuracy score,\n",
    "cross-validation, ROC curve and feature importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60970268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit(base_dt, x_train, y_train, features, performCV=False)\n",
    "log.info(f\"Accuracy on test set: {base_dt.score(x_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cef0a2",
   "metadata": {},
   "source": [
    "#### Tunning Decision tree model  With Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34f731",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "log.info(\"Decision tree with Classifier\")\n",
    "params = {\"max_depth\": np.arange(2, 7), \"criterion\": [\"gini\", \"entropy\"]}\n",
    "tree_estimator = tree.DecisionTreeClassifier()\n",
    "\n",
    "grid_tree = GridSearchCV(\n",
    "    tree_estimator, params, cv=kfold, scoring=\"accuracy\", n_jobs=1, verbose=False\n",
    ")\n",
    "\n",
    "grid_tree.fit(x_train, y_train)\n",
    "best_est = grid_tree.best_estimator_\n",
    "log.info(best_est)\n",
    "log.info(grid_tree.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7199ed",
   "metadata": {},
   "source": [
    "#### Summarize results and choose best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d1b8b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "log.info(f\"Best: {grid_tree.best_score_} using {grid_tree.best_params_}\")\n",
    "means = grid_tree.cv_results_[\"mean_test_score\"]\n",
    "stds = grid_tree.cv_results_[\"std_test_score\"]\n",
    "params = grid_tree.cv_results_[\"params\"]\n",
    "\n",
    "# Store best parameters\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    log.info(f\"{mean} ({stdev}) with: {param}\")\n",
    "best_params = grid_tree.best_params_\n",
    "\n",
    "md(\n",
    "    \"the best Hyperparameters for our Decision tree\"\n",
    "    + f\"model using gridsearch cv is {best_params}\"\n",
    ")\n",
    "max_depth_ = best_params[\"max_depth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb11ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dt = tree.DecisionTreeClassifier(max_depth=max_depth_, criterion=\"gini\")\n",
    "model_fit(base_dt, x_train, y_train, features, printFeatureImportance=True)\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafefb79",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Feature selection\n",
    "Based on feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## visualize feature importance\n",
    "threshold = 0.01\n",
    "feature_importance(base_dt, features, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c44223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **features with less than 1% of feature importance**\n",
    "n_features = list(compress(features, base_dt.feature_importances_ >= threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e88071",
   "metadata": {},
   "source": [
    "New sets with only the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7491b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sl, x_test_sl, y_train_sl, y_test_sl = train_test_split(\n",
    "    train_df.loc[:, train_df[n_features].columns], train_df[\"RET\"], random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33430776",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Decision tree tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb7429",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt = tree.DecisionTreeClassifier(max_depth=max_depth_, criterion=\"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Fitting with train set\")\n",
    "model_fit(grid_dt, x_train_sl, y_train_sl, n_features, printFeatureImportance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Fitting with test set\")\n",
    "model_fit(\n",
    "    grid_dt,\n",
    "    x_test_sl,\n",
    "    y_test_sl,\n",
    "    n_features,\n",
    "    printFeatureImportance=False,\n",
    "    roc=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6715b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test dataframe\n",
    "test_df = test_df[n_features]\n",
    "prediction = grid_dt.predict(test_df)\n",
    "log.info(f\"{prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04540d50",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier\n",
    "* HistGradientBoostingClassifier used as it is faster than GradientBoostingClassifier\n",
    "* All the features are used\n",
    "* Remove parameters not accepted by HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367c747",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# **Tunning parameters with gridsearch**\n",
    "# Remove parameters not accepted by HistGradientBoostingClassifier\n",
    "\n",
    "# Set default parameters based on classifier type\n",
    "gbm_classifier = HistGBClassifierGridSearch()\n",
    "gbm_classifier.run(x_train, y_train)\n",
    "model_fit(\n",
    "    gbm_classifier.model,\n",
    "    x_train_sl,\n",
    "    y_train_sl,\n",
    "    n_features,\n",
    "    roc=True,\n",
    "    printFeatureImportance=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdea08",
   "metadata": {},
   "source": [
    "HistGradientBoostingClassifier\n",
    "2025-01-04 16:58:12,969 - INFO - Accuracy : 0.5785\n",
    "\n",
    "2025-01-04 16:58:19,405 - INFO - Cross Validation Accuracy: 0.53241 (+/- 0.00)\n",
    "\n",
    "TA indicators simple v1\n",
    "\n",
    "2025-01-06 10:13:11,081 - INFO - Accuracy : 0.6215\n",
    "\n",
    "2025-01-06 10:13:24,791 - INFO - Cross Validation Accuracy: 0.56680 (+/- 0.00)\n",
    "\n",
    "Ta indicators parametrzied v2 (overfitting)\n",
    "\n",
    "2025-01-07 19:19:10,628 - INFO - Accuracy : 0.6517\n",
    "\n",
    "2025-01-07 19:19:15,360 - INFO - Cross Validation Accuracy: 0.51616 (+/- 0.00)\n",
    "\n",
    "GradientBoostingClassifier\n",
    "\n",
    "2025-01-04 17:34:54,242 - INFO - Accuracy : 0.6225\n",
    "\n",
    "2025-01-04 17:51:19,832 - INFO - Cross Validation Accuracy: 0.53658\n",
    "\n",
    "Ta indicators v3 (feature selection)\n",
    "\n",
    "2025-01-08 10:33:25,881 - INFO - Accuracy : 0.5866\n",
    "\n",
    "2025-01-08 10:33:34,478 - INFO - Cross Validation Accuracy: 0.52654 (+/- 0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca6df2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Parameters tuning :\n",
    "Steps:\n",
    "1. n_estimators (30-80): Number of boosting stages to perform\n",
    "2. max_depth (5-15): Maximum depth of individual trees\n",
    "3. min_samples_split (400-1000): Minimum samples required to split internal node\n",
    "4. min_samples_leaf (40): Minimum samples required at leaf node\n",
    "5. max_features (7-20): Number of features to consider for best split\n",
    "6. Fixed parameters:\n",
    "   - learning_rate=0.1\n",
    "   - subsample=0.8\n",
    "   - random_state=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32443c01",
   "metadata": {},
   "source": [
    "### Run sequential parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e78144",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_result = gbm_classifier.tune_n_estimators(x_train, y_train)\n",
    "\n",
    "tree_params_result = gbm_classifier.tune_tree_params(\n",
    "    x_train, y_train, {**n_estimators_result.best_params_}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cda803",
   "metadata": {},
   "source": [
    "Results from previous run (HistGradientBoostingClassifier):\n",
    "\n",
    "Best: 0.5368177574059928 using {'max_depth': 9, 'min_samples_leaf': 50}\n",
    "\n",
    "Best (simple TA): 0.578087598675834 using {'max_depth': 11, 'min_samples_leaf': 50}\n",
    "\n",
    "Results from previous run (GradientBoostingClassifier):\n",
    "\n",
    "Best: 0.540790 using {'max_depth': 15, 'min_samples_split': 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36adbf2c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "leaf_params_result = gbm_classifier.tune_leaf_params(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    {**n_estimators_result.best_params_, **tree_params_result.best_params_},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b42261",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Best (simple TA): 0.578087598675834 using {'l2_regularization': 0.001}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048325d",
   "metadata": {},
   "source": [
    "#### Use the model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f48346",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "max_features_result = gbm_classifier.tune_max_features(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    {\n",
    "        **n_estimators_result.best_params_,\n",
    "        **tree_params_result.best_params_,\n",
    "        **leaf_params_result.best_params_,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9a0b5",
   "metadata": {},
   "source": [
    "Best (simple TA): 0.578087598675834 using {'max_bins': 255}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37368a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model_fit(\n",
    "    max_features_result.best_estimator_,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    roc=True,\n",
    "    printFeatureImportance=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdea0ef",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Neural Network\n",
    "* Standardization of the data\n",
    "* Initialize model, loss function and optimizer\n",
    "Convert data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401cdf7",
   "metadata": {
    "title": "[Markdown]"
   },
   "outputs": [],
   "source": [
    "# #### Decisions about architecture:\n",
    "# * Tanh has been used in the first two layers because it outputs values from -1 to 1, which can be beneficial for centered data.\n",
    "# * ReLU is used in the later layers since it helps to mitigate the vanishing gradient problem and enhances computational efficiency.\n",
    "# * Sigmoid in the final layer is used for binary classification tasks, where you want to output a probability between 0 and 1.\n",
    "# * Dropout is used to prevent overfitting.\n",
    "# * The use of different activation functions (Tanh, ReLU, Sigmoid) introduces non-linearity into the model.\n",
    "# * Tanh is typically used in the first two layers because it outputs values between -1 and 1, which can be beneficial for centered data.\n",
    "# * ReLU (Rectified Linear Unit) is generally used in the later layers since it helps to mitigate the vanishing gradient problem and enhances computational efficiency.\n",
    "\n",
    "# Layers:\n",
    "# * 5 layers\n",
    "# * 100 neurons in the first layer (Tanh)\n",
    "# * 50 neurons in the second layer (Tanh) with dropout (0.33)\n",
    "# * 150 neurons in the third layer (ReLU)\n",
    "# * 50 neurons in the fourth layer (ReLU)\n",
    "# * 35 neurons in the fifth layer (Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1da82",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Preparing standardization and normalization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train[features])\n",
    "X_train = scaler.fit_transform(x_train[features])\n",
    "scaler.fit(x_test[features])\n",
    "X_test = scaler.fit_transform(x_test[features])\n",
    "nn_model = Net(len(features))\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters())\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "Y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf0f3a",
   "metadata": {},
   "source": [
    "Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c2f85",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "batch_size = 5000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(train_df), batch_size):\n",
    "        batch_X = X_train_tensor[i : i + batch_size]\n",
    "        batch_y = y_train_tensor[i : i + batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nn_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y.view(-1, 1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = nn_model(X_test_tensor)\n",
    "    # Convert probabilities to binary predictions using 0.5 threshold\n",
    "    y_predict = (outputs >= 0.5).squeeze().numpy()\n",
    "\n",
    "print(classification_report(Y_test_tensor, y_predict, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d8c4a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Model comparison plot\n",
    "Convert model results to DataFrame for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd687c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {\n",
    "    \"Decision Tree (Base)\": base_dt.score(x_test, y_test),\n",
    "    \"Decision Tree (Tuned)\": grid_dt.score(x_test_sl, y_test),\n",
    "    \"GB (n_estimators)\": n_estimators_result.best_estimator_.score(x_test, y_test),\n",
    "    \"GB (+ tree params)\": tree_params_result.best_estimator_.score(x_test, y_test),\n",
    "    \"GB (+ leaf params)\": leaf_params_result.best_estimator_.score(x_test, y_test),\n",
    "    \"GB (+ max features)\": max_features_result.best_estimator_.score(x_test, y_test),\n",
    "    \"Neural Network\": accuracy_score(Y_test_tensor, y_predict),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88e4ad",
   "metadata": {},
   "source": [
    "Create dictionary of model results including stepping stone models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_accuracy(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aedfb4",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "Simulation of actual returns based on predictions coming from models.\n",
    "It must be stated that there are a few assumptions:\n",
    "* Zero transaction costs\n",
    "* No slippage (no market impact)\n",
    "* Unconstrained shorting\n",
    "Takeaways:\n",
    "- If, on average, we are right >50% of the time, and the sizing of the trade is constant,\n",
    "then we can expect to make money. Hence, the line with some positive drift is expected.\n",
    "- The slope of this line depends on the accuracy of the model. The higher the accuracy, the higher the slope.\n",
    "- As previously stated, this is a very simplified model and does not take into account many factors\n",
    "that could affect the real performance of the strategy.\n",
    "- The scope of this project is limited, i.e. to generate a buy/sell signal that in real application\n",
    "is just a small part of actual trading decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b06f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "simulate_strategy(Y_test_tensor, y_predict, n_simulations=1, n_days=100)\n",
    "\n",
    "# plot simulation\n",
    "# Create dataframe for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44468d1",
   "metadata": {},
   "source": [
    "### Key Findings:\n",
    "\n",
    "- All models are compared against the benchmark accuracy of 51.31%\n",
    "- The tuned Gradient Boosting model significantly outperforms other models\n",
    "- Hyperparameter tuning improved both Decision Tree and Gradient Boosting performance\n",
    "- Gradient Boosting shows superior performance compared to Decision Trees, which is expected\n",
    "- HistGradientBoostingClassifier is much faster than GradientBoostingClassifier without\n",
    "  much compromising the performance\n",
    "- Further improvement in out-of-sample performance is possible by both\n",
    "better feature engineering and further hyperparameter tuning\n",
    "   * More technical indicators could be introduced (e.g. ROC, Golden Cross, etc.)\n",
    "   * More variables based on the categorical variables (which are dropped as of now)\n",
    "could bring in some value\n",
    "- Even simple technical indicators can improve the performance of the model more than right choice of hyperparameters\n",
    "- Using too many features caused extreme overfitting (expected)\n",
    "- Incorrectly calculated technical indicators had some predictive power (unexpected)\n",
    "- Neural network-based model is not able to beat the benchmark accuracy of 51.31% (NN was only marginally better)\n",
    "- More sophisticated MLOps tools would be useful to track the performance of the model and the changes in the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
