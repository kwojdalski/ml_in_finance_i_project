{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from gics.definitions import d_20180929"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc647f9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Before this workflow I tried different models with decent but not spectacular results (my best score was **0.5158**).\n",
    "Experimenting with models included XGBoost, Catboost, Adaboost, lightgbm in different configuration based on\n",
    "optuna hyperparametrization. On top of it, I engineered new features coming from technical analysis.\n",
    "In general, I started being skeptical about ability to generate a great model for this particular competition once I went through\n",
    "[this](https://github.com/vitoriarlima/stock-returns) particular notebook, which includes incorrectly implemented features, but still somehow managed to land a 17th percentile\n",
    "position in the competition.\n",
    "\n",
    "This particular notebook, which significantly beats all my previous submissions in the competition,\n",
    "is based on the following idea - what would happen if I could\n",
    "identify stocks that were part of the dataset? Identyfing even a small subset of\n",
    "symbols could significantly boost my position in the competition as all results are very compressed\n",
    "(between 0.5 and 0.5287). It would take only approx. 5.2k correctly identyfied observations to be no. 1.\n",
    "Why 5k? Test set consists of 198k, expected value of correctly identified returns is **2.6k** (out of **5.2k**).\n",
    "So I need just above 2.6k net correct answers to be no. 1 (>~130bps). I think that should be doable with the method I assumed is worth\n",
    "trying.\n",
    "\n",
    "# What's the method then?\n",
    "1. Load anonymized train dataset\n",
    "2. Analyze industry, sector, industry group, sub-industry breakdown and distribution\n",
    "3. Compare it with some standardized methodology (e.g. GICS).\n",
    "4. Try to identify sector / industry / sub-industry / industry group for some potential symbol\n",
    "that I had high conviction of existed over the last 30 years or so (e.g. Microsoft, Berkshire Hathaway, etc.).\n",
    "5. I decided to go with MSFT (more on that later)\n",
    "6. Get MSFT daily returns\n",
    "7. Fit them to returns coming from anonymized train dataset (but only for subindustry Microsoft is expected to be in).\n",
    "8. Calculate max rolling correlation between MSFT daily returns and observations in the anonymized train dataset.\n",
    "If it's nearly 1 for a couple of observations, then we can be sure that stock id for that particular observation\n",
    "could be mapped to MSFT.\n",
    "9. On top of it, I can map dates to DATE column in the anonymized train dataset.\n",
    "10. Once I identify stocks and dates in train dataset, I can just do the same for test dataset.\n",
    "11. The last step is to check returns for RET in test dataset with use of market data, remap it to 1s and 0s.\n",
    "12. Load the best model I had hands on (the 0.5158 one) and found values from step 11 onto it.\n",
    "13. Submit!\n",
    "**Caveats:**\n",
    "This method obviously could perform arbitrarily better if I could work more on reference data (take into account delistings, etc.) to get more symbols from the past.\n",
    "I thought that my current result (first attempt with this method) is good enough to win the competition, so I stopped there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "log.info(\"Starting Guess Industry\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19591c9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66eff6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "out12 = pickle.load(open(\"data/05_model_input/train_df_merged.pkl\", \"rb\"))\n",
    "out12 = out12.loc[:, ~out12.columns.duplicated()]\n",
    "date_dict = pd.read_pickle(\"data/01_raw/mapped_dates.pkl\")\n",
    "out12[\"DT\"] = out12[\"DATE\"].map(date_dict)\n",
    "out12.groupby(\"DT\")[\"STOCK\"].nunique().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e5662",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Check how many unique values there are for each column\n",
    "out12[\"INDUSTRY\"].nunique()\n",
    "out12[\"INDUSTRY_GROUP\"].nunique()\n",
    "out12[\"SECTOR\"].nunique()\n",
    "out12[\"SUB_INDUSTRY\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcafa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of unique sub-industries per industry\n",
    "sector_counts = (\n",
    "    out12.groupby(\"SECTOR\")[\"INDUSTRY_GROUP\"].nunique().sort_values(ascending=False)\n",
    ")\n",
    "industry_group_counts = (\n",
    "    out12.groupby(\"INDUSTRY_GROUP\")[\"INDUSTRY\"].nunique().sort_values(ascending=False)\n",
    ")\n",
    "log.info(\"\\nNumber of unique sub-industries per industry:\")\n",
    "for sector, count in sector_counts.items():\n",
    "    log.info(f\"{sector}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067e664",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Found mappings for GICS classification\n",
    "# 0 - Energy\n",
    "# 1 - Materials\n",
    "# 2 - ?\n",
    "# 3 - Industrials\n",
    "# 4 - Consumer Discretionary\n",
    "# 5 - Consumer Staples\n",
    "# 6 - Health Care\n",
    "# 7 - Financials\n",
    "# 8 - Information Technology\n",
    "# 9 - Communication Services\n",
    "# 10 - Utilities\n",
    "# 11 - Real Estate\n",
    "\n",
    "# MSFT\n",
    "# Sector -Information Technology\n",
    "# Industry - Software\n",
    "# IndustryGroup - Software & Services\n",
    "# SubIndustry - Systems Software\n",
    "\n",
    "# Print the structure of GICS classification levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c51dce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Split GICS classifications by level based on key length\n",
    "sectors = {k: v[\"name\"] for k, v in d_20180929.items() if len(k) == 2}\n",
    "industry_groups = {k: v[\"name\"] for k, v in d_20180929.items() if len(k) == 4}\n",
    "industries = {k: v[\"name\"] for k, v in d_20180929.items() if len(k) == 6}\n",
    "sub_industries = {k: v[\"name\"] for k, v in d_20180929.items() if len(k) == 8}\n",
    "log.info(\"Comparison data vs GICS:\")\n",
    "log.info(f\"uniq industries: {out12['INDUSTRY'].nunique()} vs {len(industries)}\")\n",
    "log.info(f\"ind groups: {out12['INDUSTRY_GROUP'].nunique()} vs {len(industry_groups)}\")\n",
    "log.info(f\"sectors: {out12['SECTOR'].nunique()} vs {len(sectors)}\")\n",
    "log.info(f\"sub-ind: {out12['SUB_INDUSTRY'].nunique()} vs {len(sub_industries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6692958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for each GICS level\n",
    "# Create mappings from sub-industry to higher levels\n",
    "sub_industry_mappings = {}\n",
    "for sub_ind_code, sub_ind_name in sub_industries.items():\n",
    "    sector_code = sub_ind_code[:2]\n",
    "    industry_group_code = sub_ind_code[:4]\n",
    "    industry_code = sub_ind_code[:6]\n",
    "\n",
    "    sub_industry_mappings[sub_ind_code] = {\n",
    "        \"sector\": sectors[sector_code],\n",
    "        \"industry_group\": industry_groups[industry_group_code],\n",
    "        \"industry\": industries[industry_code],\n",
    "        \"sub_industry\": sub_ind_name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81cc5aa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Count sub-industries per sector\n",
    "sector_industry_group_counts = {}\n",
    "for industry_group_code in industry_groups:\n",
    "    sector_code = industry_group_code[:2]\n",
    "    sector_name = sectors[sector_code]\n",
    "    sector_industry_group_counts[sector_name] = (\n",
    "        sector_industry_group_counts.get(sector_name, 0) + 1\n",
    "    )\n",
    "\n",
    "sector_counts.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6da1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like two distribution are very similar\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "pd.Series(sector_industry_group_counts).plot(\n",
    "    kind=\"bar\", ax=ax, alpha=0.5, label=\"Industry Groups\"\n",
    ")\n",
    "sector_counts.sort_index(ascending=True).plot(\n",
    "    kind=\"bar\", ax=ax, alpha=0.5, label=\"Sectors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705055e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Let's try to find MSFT! It must be somewhere in the data\n",
    "# Get date range\n",
    "end_date = datetime(2020, 10, 10)\n",
    "start_date = end_date - timedelta(days=25 * 365)  # Approximately 25 years, just a guess\n",
    "\n",
    "# Download MSFT data and calculate MSFT daily returns\n",
    "msft = yf.download(\"MSFT\", start=start_date, end=end_date, progress=False)\n",
    "msft_returns = msft[\"Close\"].pct_change().dropna()\n",
    "# Display first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889af6cc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "### try to identify msft\n",
    "# It turns out that msft is in the sub-industry of \"Systems Software\"\n",
    "# All I need to do is find one symbol that would allow me to narrow down further data\n",
    "# search\n",
    "# Ok, I managed to guess where MSFT could be (manually)\n",
    "# out14 = out12\n",
    "# test_df.set_index(\"ID\", inplace=True)\n",
    "# out12 = test_df\n",
    "out13 = out12[\n",
    "    (out12[\"SECTOR\"] == 8)\n",
    "    & (out12[\"INDUSTRY_GROUP\"] == 20)\n",
    "    & (out12[\"INDUSTRY\"] == 57)\n",
    "    & (out12[\"SUB_INDUSTRY\"] == 142)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting returns and anonymized dates\n",
    "ret_cols = [\n",
    "    col for col in out13.columns if col.startswith(\"RET_\") and col[4:].isdigit()\n",
    "]\n",
    "df = out13[ret_cols + [\"DATE\", \"STOCK\"]]\n",
    "df = df.transpose()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate max rolling correlation\n",
    "def calc_max_rolling_corr(series, yfin_returns, window=20, return_date=False):\n",
    "    \"\"\"Calculate maximum rolling correlation between two time series.\n",
    "\n",
    "    Args:\n",
    "        series: First time series to compare (anonymized returns)\n",
    "        yfin_returns: Second time series from Yahoo Finance to compare against\n",
    "        window: Size of rolling window to use for correlation calculation\n",
    "        return_date: If True, also return the date of maximum correlation\n",
    "\n",
    "    Returns:\n",
    "        If return_date is False:\n",
    "            float: Maximum absolute correlation value found\n",
    "        If return_date is True:\n",
    "            tuple: (Maximum correlation value, Date of maximum correlation)\n",
    "    \"\"\"\n",
    "    # Convert series to numpy for faster computation\n",
    "    series_np = series.values\n",
    "    yfin_np = yfin_returns.values.flatten()\n",
    "\n",
    "    max_corr = -1\n",
    "    n_yfin = len(yfin_np)\n",
    "    best_date = None\n",
    "\n",
    "    # Only need to check if we have enough data points\n",
    "    if n_yfin >= window:\n",
    "        # For each possible window in MSFT returns\n",
    "        for i in range(n_yfin - window + 1):\n",
    "            window_returns = yfin_np[i : i + window]\n",
    "            corr = np.corrcoef(series_np, window_returns)[0, 1]\n",
    "            if abs(corr) > max_corr:\n",
    "                max_corr = abs(corr)\n",
    "                if return_date:\n",
    "                    # best_date should be the last day of the window\n",
    "                    # TODO not sure about this one\n",
    "                    best_date = yfin_returns.index[i + window - 1]\n",
    "\n",
    "    if return_date:\n",
    "        return max_corr, best_date\n",
    "    return max_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99913c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate max corelations for MSFT\n",
    "max_correlations = {}\n",
    "\n",
    "for i in range(len(df.columns)):\n",
    "    max_correlations[df.columns[i]] = calc_max_rolling_corr(\n",
    "        df.iloc[2:, i], msft_returns, return_date=True\n",
    "    )\n",
    "msft_correlations = pd.DataFrame(max_correlations).T\n",
    "msft_correlations.columns = [\"corr\", \"date\"]\n",
    "high_correlations = msft_correlations.sort_values(\"corr\", ascending=False).head(65)\n",
    "high_correlations[high_correlations.index == 43259.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21cacd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "run = False\n",
    "if run:\n",
    "    merged_df = pd.merge(high_correlations, out13, left_index=True, right_on=\"ID\")\n",
    "    # Create dictionary mapping between date and DATE columns\n",
    "    # date_dict2 = dict(zip(merged_df[\"DATE\"], merged_df[\"date\"]))\n",
    "    date_dict = dict(zip(merged_df[\"DATE\"], merged_df[\"date\"]))\n",
    "    # Merge date_dict and date_dict2 since they contain the same mapping\n",
    "    date_dict.update(date_dict2)\n",
    "    # datedict2 was for test_df\n",
    "    # Anyway, here's mapping ~220 dates\n",
    "    # pd.to_pickle(date_dict, \"data/01_raw/mapped_dates.pkl\")\n",
    "\n",
    "\n",
    "# Based on the result of this loop (correlation of nearly 1, we can conclude that,\n",
    "# for instance, observation 43259.0 is probably MSFT.)\n",
    "# In theory, it could be something else, but extremely highly correlated (like CFD contract, SSF, etc.)\n",
    "# So yeah, I managed to decode MSFT. Now let's try to decode more stock ids of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84322cfa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# I managed to identify MSFT by stock id as 1720\n",
    "# Now let's check if all returns can be identified by stock id, not just for single date\n",
    "# I use df as it's highly narrowed down (instead of using 300k or so observations)\n",
    "# I have to iterate over just 155 observations (which takes ~5 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89010f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to do that we need to define two additional functions\n",
    "def process_column(col, df, yfin_df):\n",
    "    \"\"\"Process a single column to find correlation with MSFT returns.\"\"\"\n",
    "    stock_id = df[col].iloc[0]\n",
    "    date_idx = df[col].iloc[1]\n",
    "    max_corr, date = calc_max_rolling_corr(df[col].iloc[2:], yfin_df, return_date=True)\n",
    "    return {\"date_idx\": date_idx, \"stock_id\": stock_id, \"date\": date, \"corr\": max_corr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_columns(df: pd.DataFrame, yfin_df: pd.Series):\n",
    "    \"\"\"Process all columns and track results.\n",
    "\n",
    "    Iterates through columns in the dataframe, calculating correlations with subesquent columns of yfin_df.\n",
    "    Tracks results in both a list and dataframe format, printing progress and highest\n",
    "    correlations periodically.\n",
    "\n",
    "    Args:\n",
    "        df: Input dataframe containing columns to process\n",
    "        yfin_df: Yahoo Finance dataframe with MSFT returns to compare against\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - List of dictionaries with correlation results for each column\n",
    "            - DataFrame containing the same correlation results in tabular format\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    all_results = []\n",
    "    date_df = pd.DataFrame()\n",
    "\n",
    "    # Process each column in returns DataFrame\n",
    "    for i in range(len(yfin_df.columns)):\n",
    "        if i % 20 == 0:\n",
    "            log.info(f\"Processing {i} / {len(yfin_df.columns)}\")\n",
    "\n",
    "        # Process each column in input DataFrame\n",
    "        for col in df.columns:\n",
    "            result = process_column(col, df, pd.DataFrame(yfin_df.iloc[:, i]))\n",
    "            results.append(result)\n",
    "\n",
    "            # Add to date_df\n",
    "            result_df = pd.DataFrame([result])\n",
    "            result_df[\"symbol\"] = yfin_df.columns[i]\n",
    "            result_df[\"id\"] = col\n",
    "            date_df = pd.concat([date_df, result_df], ignore_index=True)\n",
    "\n",
    "            # Print results\n",
    "            log.debug(f\"{col}: {result['corr']} on {result['date']}\")\n",
    "\n",
    "        all_results.append(date_df)\n",
    "        date_df = pd.DataFrame()  # Reset for next iteration\n",
    "\n",
    "        # Print highest correlations periodically\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            log.info(\"\\nHighest correlations:\")\n",
    "            correlations = pd.Series({r[\"date_idx\"]: r[\"corr\"] for r in results})\n",
    "            log.info(correlations.sort_values(ascending=False).head())\n",
    "\n",
    "    return results, pd.concat(all_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457827a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Process all columns for msft\n",
    "# This function is designed in such a way that it can process more than one set of returns\n",
    "# from yahoo finance. In this case, I'm passing only one set (MSFT),\n",
    "# but further on, we will get to more generalized approach\n",
    "results, date_df = process_all_columns(df.iloc[:, 0:10], msft_returns)\n",
    "\n",
    "\n",
    "# All observations can be identified( correlations are very close to 0)\n",
    "# and comes from date range from 2010-02-09 to 2017-12-18\n",
    "date_df.date.min()\n",
    "date_df.date.max()\n",
    "\n",
    "\n",
    "# Get one of the dates\n",
    "# In my case it's 2016-07-20\n",
    "# which seems to map to 97\n",
    "# In other words date_idx = 97 => 2016-07-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733fc858",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Ok, now let's try to identify other stocks for this particular date\n",
    "# I'll use S&P 500, Dow Jones, and NASDAQ 100 ticker lists\n",
    "# I'll use yfinance to get the data\n",
    "# Obviously, it's probably not all, but good enough to win the competition. If I could identify\n",
    "# just 50% of the stocks and observations, it would give me an edge over the competitors.\n",
    "# I would score at least 0.75 (100% accuracy over 50% of observations and 50% over the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614658cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_syms(exclude_mapped=False):\n",
    "    sec_tickers = pd.read_json(\n",
    "        \"./data/01_raw/company_tickers.json\"\n",
    "    ).T  # https://www.sec.gov/files/company_tickers.json\n",
    "    sec_tickers.drop(columns=[\"cik_str\"], inplace=True)\n",
    "    sec_tickers.rename(columns={\"ticker\": \"Symbol\", \"title\": \"Name\"}, inplace=True)\n",
    "    symbols = list(sec_tickers[\"Symbol\"].unique())\n",
    "    if exclude_mapped:\n",
    "        mapped_stocks = pd.read_pickle(\"./data/01_raw/mapped_stocks.pkl\")\n",
    "        symbols = [sym for sym in symbols if sym not in mapped_stocks.values()]\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prices for all S&P 500 stocks on 2016-07-20 using yfinance\n",
    "# Download prices for all constituents on target date\n",
    "# Use list of symbols and handle potential errors\n",
    "symbols = get_yahoo_syms()\n",
    "end_date = datetime(2016, 7, 22)\n",
    "start_date = end_date - timedelta(days=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19809094",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Now I want to download ticks more systematically than on the previous call (just for MSFT)\n",
    "# We must be careful as there are limits on the number of requests per hour / daily (afaik)\n",
    "# So I'll download 100 symbols at a time\n",
    "\n",
    "all_returns = pd.DataFrame()\n",
    "for file in os.listdir(\"./data/01_raw/yahoo_data\"):\n",
    "    print(f\"Processing {file}\")\n",
    "    prices = pd.read_pickle(f\"./data/01_raw/yahoo_data/20160722/{file}\")\n",
    "    returns = prices[\"Close\"].pct_change()\n",
    "    cols = returns[1:].isna().any(axis=0)\n",
    "    returns = returns.loc[:, ~cols]\n",
    "    returns = returns.loc[:, ~returns.columns.duplicated()]\n",
    "    all_returns = pd.concat([all_returns, returns], axis=1)\n",
    "    all_returns = all_returns.loc[:, ~all_returns.isna().all()]\n",
    "# Check for any NaN values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad91b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_stocks(all_results_df, correlation_threshold=0.99):\n",
    "    \"\"\"Create mapping of highly correlated stocks above threshold.\"\"\"\n",
    "    dict_ = {}\n",
    "    for _, row in all_results_df[\n",
    "        all_results_df[\"corr\"] > correlation_threshold\n",
    "    ].iterrows():\n",
    "        stock_id = row[\"stock_id\"]\n",
    "        stock_name = row[\"symbol\"]\n",
    "        dict_[stock_id] = stock_name\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c66a1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Now let's narrow down dataset to one date\n",
    "out13_20160720 = out12[out12.DATE == 97]\n",
    "# Ok, it has 29 observations which will allow to potentially identify 29 stocks\n",
    "tmp2 = out13_20160720[ret_cols + [\"DATE\", \"STOCK\"]]\n",
    "df2 = tmp2.transpose()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fc552",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(df2.columns), 10):\n",
    "    print(f\"Processing {i} / {len(df2.columns)}\")\n",
    "    _, all_results_df = process_all_columns(df2.iloc[:, i : i + 10], all_returns)\n",
    "    all_results_df.sort_values(\"corr\", ascending=False).groupby(\"date_idx\").head(50)\n",
    "    # Create mapping of highly correlated stocks (>0.99)\n",
    "    mapped_stocks = map_stocks(all_results_df)\n",
    "    # Load existing mapping if it exists, otherwise start with empty dict\n",
    "    if os.path.exists(\"data/01_raw/mapped_stocks.pkl\"):\n",
    "        existing_mapped_stocks = pd.read_pickle(\"data/01_raw/mapped_stocks.pkl\")\n",
    "        existing_mapped_stocks.update(mapped_stocks)\n",
    "        pd.to_pickle(existing_mapped_stocks, \"data/01_raw/mapped_stocks.pkl\")\n",
    "    else:\n",
    "        pd.to_pickle(mapped_stocks, \"data/01_raw/mapped_stocks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd19220",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = get_yahoo_syms(exclude_mapped=True)\n",
    "len(symbols)\n",
    "\n",
    "# Get prices for all S&P 500 stocks on 2016-07-20 using yfinance\n",
    "# Download prices for all constituents on target date\n",
    "# Use list of symbols and handle potential errors\n",
    "end_date = datetime(2016, 7, 22)\n",
    "start_date = end_date - timedelta(days=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to download ticks more systematically than on the previous call (just for MSFT)\n",
    "# We must be careful as there are limits on the number of requests per hour / daily (afaik)\n",
    "# So I'll download 100 symbols at a time\n",
    "def download_batch_prices(\n",
    "    symbols: list, start_date: datetime, end_date: datetime\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Download stock prices in batches for a list of symbols.\n",
    "\n",
    "    Args:\n",
    "        symbols: List of stock symbols to download\n",
    "        start_date: Start date for price data\n",
    "        end_date: End date for price data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing downloaded price data\n",
    "    \"\"\"\n",
    "    prices = pd.DataFrame()\n",
    "    for i in range(0, len(symbols), 100):\n",
    "        batch_symbols = symbols[i : i + 100]\n",
    "        try:\n",
    "            batch_prices = yf.download(\n",
    "                batch_symbols,\n",
    "                start=start_date,\n",
    "                end=end_date,\n",
    "                progress=True,\n",
    "            )\n",
    "            if prices.empty:\n",
    "                prices = batch_prices\n",
    "            else:\n",
    "                prices = pd.concat([prices, batch_prices], axis=1)\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error downloading batch {i}-{i+100}: {e}\")\n",
    "            continue\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List files in the yahoo data directory\n",
    "data_dir = \"./data/01_raw/yahoo_data\"\n",
    "\n",
    "files = os.listdir(data_dir)\n",
    "files = [f.replace(\".pkl\", \"\") for f in files if f.endswith(\".pkl\")]\n",
    "if __name__ == \"__main__\":\n",
    "    for sym in symbols:\n",
    "        if sym in files:\n",
    "            print(f\"{sym} already exists\")\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"Downloading {sym}\")\n",
    "            prices = download_batch_prices([sym], start_date, end_date)\n",
    "            sleep(1)\n",
    "            prices.to_pickle(f\"./data/01_raw/yahoo_data/{sym}.pkl\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {sym}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b6fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
